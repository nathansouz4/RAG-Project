{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text=text._result.candidates[0].content.parts[0].text\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('app/src/shared/.env')\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"whats the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat()\n",
    "chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Hello,my name is Nathan!\")\n",
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Whats my name?\")\n",
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in chat.history:\n",
    "  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use LangChain to Access Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI,ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "result = llm.invoke(\"tell me a joke in french\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-pro\", google_api_key=GOOGLE_API_KEY)\n",
    "result = llm.invoke(\"tell me a joke in french\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "result = llm.invoke(\"tell me a joke in portuguese\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini LangChain QA using RAG structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auth with gemini api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text=text._result.candidates[0].content.parts[0].text\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "\n",
    "load_dotenv('app/src/shared/.env')\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document loaders - Load Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"test/nr-10.pdf\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"test\")\n",
    "pdf_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pages\n",
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(pdf_docs):\n",
    "  \"\"\"\n",
    "  Extracts the text from all pages in a list of documents.\n",
    "\n",
    "  Args:\n",
    "    docs: A list of \"Document\" objects.\n",
    "\n",
    "  Returns:\n",
    "    A string containing the concatenated text of all pages.\n",
    "  \"\"\"\n",
    "\n",
    "  text = \"\"\n",
    "  for doc in pdf_docs:\n",
    "    text += doc.page_content \n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(pdf_docs)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning the pdf docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "  \"\"\"\n",
    "  Removes markup and special characters from the text.\n",
    "\n",
    "  Args:\n",
    "    text: A string containing the text to be cleaned.\n",
    "\n",
    "  Returns:\n",
    "    A string with the cleaned text.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Converte o texto para minúsculas.\n",
    "  text = text.lower()\n",
    "\n",
    "  # Remove caracteres que não sejam letras, números ou espaços em branco.\n",
    "  # (Mantém apenas letras, números e espaços em branco.)\n",
    "  text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "  # Substitui sequências de espaços em branco por um único espaço.\n",
    "  # (Garante espaçamento consistente e evita chunks indesejados.)\n",
    "  text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "  # Return the cleaned text\n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage\n",
    "text_cleaned = clean_text(text)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting - Chunking the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# using recursive text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Semantic Chunking\n",
    "# text_splitter = SemanticChunker(gemini_embeddings, breakpoint_threshold_type=\"percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crunks = text_splitter.split_documents(pdf_docs)\n",
    "print(f'### Chunk 1: \\n\\n{crunks[0].page_content}\\n\\n=====\\n')\n",
    "print(f'### Chunk 2: \\n\\n{crunks[1].page_content}\\n\\n=====')\n",
    "print(len(crunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks  = text_splitter.create_documents([text_cleaned])\n",
    "print(f'### Chunk 1: \\n\\n{crunks[0].page_content}\\n\\n=====\\n')\n",
    "print(f'### Chunk 2: \\n\\n{crunks[1].page_content}\\n\\n=====')\n",
    "print(len(crunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
    "                                                  task_type=\"retrieval_document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import Chroma\n",
    "# from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# creating a db -> vector store\n",
    "db = Chroma.from_documents(\n",
    "                     documents=pdf_docs,             # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"Em todas as intervenções em instalações elétricas devem ser adotadas medidas \n",
    "preventivas  de  controle  do  risco  elétrico\"\"\"\n",
    "# query = \"NR 10\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorstore retriver\n",
    "# retriever = db.as_retriever(\n",
    "#     search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.5}\n",
    "# )\n",
    "\n",
    "# #vectorstore retriver\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retriever.get_relevant_documents(\"NR 10\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAI\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                 temperature=0.7, top_p=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Design "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "\n",
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"O que diz o topico 10.2.8.1 da norma regulamentadora NR 10?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiQueryRetriever - fine tuning prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://www.guiatrabalhista.com.br/legislacao/nr/nr10.htm\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "splits = text_splitter.split_documents(data)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to disk\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vectordb = Chroma.from_documents(documents=splits, \n",
    "                                 embedding=gemini_embeddings,\n",
    "                                 persist_directory=\"./chroma_db\")\n",
    "\n",
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n",
    "\n",
    "question = \"O que diz o tópico 10.2.8.1 da norma regulamentadora NR 10?\"\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "# create a chain to answer questions\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    retriever=retriever_from_llm,\n",
    "    return_source_documents=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa(\"O que diz o tópico 10.2.8.1 da norma regulamentadora NR 10?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo v1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv('app/src/shared/.env')\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain libraries\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "#load docs\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "#vectorstore\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# processing docs\n",
    "#its missing cleaning text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_from_web(url: str):\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20\n",
    "    )\n",
    "    split_docs = splitter.split_documents(docs)\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_documents_from_web(\"https://www.guiatrabalhista.com.br/legislacao/nr/nr10.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'### Chunk 1: \\n\\n{docs[40].page_content}\\n\\n=====\\n')\n",
    "print(f'### Chunk 2: \\n\\n{docs[41].page_content}\\n\\n=====')\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(docs):\n",
    "    gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vectorstore = FAISS.from_documents(docs, embedding=gemini_embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(vectorstore):\n",
    "    model = GoogleGenerativeAI(\n",
    "        model=\"gemini-pro\",\n",
    "        temperature=0.3\n",
    "        )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"Responda às perguntas do usuário com base no contexto: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    # chain = prompt | model\n",
    "    chain = create_stuff_documents_chain(\n",
    "        llm=model,\n",
    "        prompt=prompt\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "    retriever_prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"human\", \"Considerando a conversa acima, gere uma consulta de pesquisa para obter informações relevantes para a conversa\")\n",
    "    ])\n",
    "\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm=model,\n",
    "        retriever=retriever,\n",
    "        prompt=retriever_prompt\n",
    "    )\n",
    "\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        # retriever,\n",
    "        history_aware_retriever,\n",
    "        chain\n",
    "    )\n",
    "\n",
    "    return retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chat(chain, question, chat_history):\n",
    "    response = chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_documents_from_web('https://www.guiatrabalhista.com.br/legislacao/nr/nr10.htm')\n",
    "vectorstore = create_db(docs)\n",
    "chain = create_chain(vectorstore)\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    response = process_chat(chain, user_input, chat_history)\n",
    "    chat_history.append(HumanMessage(content=user_input))\n",
    "    chat_history.append(AIMessage(content=response))\n",
    "\n",
    "    print(\"Assistant:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-structured RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As normas brasileiras relacionadas a instalações elétricas abrangem uma variedade de aspectos, desde baixa tensão até instalações específicas em locais como áreas classificadas e sistemas fotovoltaicos. As principais normas da Associação Brasileira de Normas Técnicas (ABNT) sobre instalações elétricas incluem:\n",
    "\n",
    "* NBR 5410 - Instalações elétricas de baixa tensão: Trata das condições para o projeto, execução e manutenção de instalações elétricas de baixa tensão.\n",
    "\n",
    "* NBR 14039 - Instalações elétricas de média tensão de 1,0 kV a 36,2 kV: Estabelece as condições para projeto, execução e manutenção dessas instalações.\n",
    "\n",
    "* NBR 5413 - Iluminância de interiores: Define os requisitos para níveis de iluminância em ambientes internos.\n",
    "\n",
    "* NBR 13570 -  Instalações Elétricas em Locais de Afluência de Público - Requisitos específicos\n",
    "\n",
    "* ABNT NBR IEC 60079-14 - Instalações elétricas em atmosferas explosivas - Área classificada: Requisitos para instalações em áreas com risco de explosão.\n",
    "\n",
    "* NBR 10898 - Sistemas de iluminação de emergência: Especifica os requisitos para sistemas de iluminação de emergência em edifícios.\n",
    "\n",
    "* NBR 15514 - Recipientes transportáveis de gás liquefeito de petróleo (GLP) — Área de armazenamento — Requisitos de segurança\n",
    "    * Embora a norma NBR 15514 não seja diretamente uma norma de instalações elétricas, ela possui interseções com a área elétrica em aspectos relacionados à segurança, especialmente devido aos riscos potenciais de explosões e incêndios associados ao GLP.\n",
    "* NBR 5419 - Proteção contra descargas atmosféricas (em quatro partes):\n",
    "\n",
    "    * Parte 1: Princípios gerais\n",
    "    * Parte 2: Gerenciamento de risco\n",
    "    * Parte 3: Danos físicos a edificações e perigos à vida\n",
    "    * Parte 4: Sistemas elétricos e eletrônicos internos na estrutura\n",
    "\n",
    "    * NBR 16280 - Reforma em edificações - Sistema de gestão de reformas: Estabelece requisitos para reformas em edificações, incluindo instalações elétricas.\n",
    "\n",
    "\n",
    "Essas são algumas das principais normas que tratam de diversos aspectos das instalações elétricas no Brasil, garantindo segurança, eficiência e conformidade técnica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "def process_pdf_file(filename, path):\n",
    "    \n",
    "    raw_pdf_elements = partition_pdf(\n",
    "        filename=os.path.join(path, filename),  # Combine path and filename\n",
    "        extract_images_in_pdf=False,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=path,\n",
    "    )\n",
    "    return raw_pdf_elements\n",
    "\n",
    "def process_single_pdf(pdf_folder_path: str, filename: str):\n",
    "    \"\"\"Helper function to process a single PDF file.\"\"\"\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        print(f\"Reading the PDF doc: {filename}\")\n",
    "        return process_pdf_file(filename, pdf_folder_path)\n",
    "    return []\n",
    "\n",
    "# Loop through PDF Files with ThreadPoolExecutor:\n",
    "def process_multiple_pdfs(pdf_folder_path: str, max_workers: int = 4) -> List:\n",
    "    \"\"\"Processes all PDF files within a specified folder using ThreadPoolExecutor.\"\"\"\n",
    "    all_elements = []\n",
    "    filenames = [filename for filename in os.listdir(pdf_folder_path) if filename.endswith(\".pdf\")]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = executor.map(lambda filename: process_single_pdf(pdf_folder_path, filename), filenames)\n",
    "    \n",
    "    for result in results:\n",
    "        all_elements.extend(result)  # Extend the all_elements list\n",
    "    \n",
    "    return all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path  = \"/Users/nathan/workspace/tcc/app/src/database/pdf/\"\n",
    "raw_pdfs_elements = process_multiple_pdfs(pdf_folder_path, max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "# Configure logging \n",
    "logging.basicConfig(filename='pdf_processing.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "def process_pdf_file(filename, path):\n",
    "    \"\"\"Processes a single PDF file and returns the extracted elements.\n",
    "\n",
    "    Logs information and errors during processing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        raw_pdf_elements = partition_pdf(\n",
    "            filename=os.path.join(path, filename),  # Combine path and filename\n",
    "            extract_images_in_pdf=False,\n",
    "            infer_table_structure=True,\n",
    "            chunking_strategy=\"by_title\",\n",
    "            max_characters=4000,\n",
    "            new_after_n_chars=3800,\n",
    "            combine_text_under_n_chars=2000\n",
    "        )\n",
    "        logging.info(f\"Successfully processed PDF: {filename}\")\n",
    "        return raw_pdf_elements\n",
    "    except Exception as e:  # Catch specific PDF processing errors\n",
    "        logging.error(f\"Error processing PDF: {filename} - {e}\")\n",
    "        return []\n",
    "\n",
    "def process_single_pdf(pdf_folder_path: str, filename: str):\n",
    "    \"\"\"Helper function to process a single PDF file.\"\"\"\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        logging.info(f\"Reading PDF doc: {filename}\")\n",
    "        return process_pdf_file(filename, pdf_folder_path)\n",
    "    return []\n",
    "\n",
    "# Loop through PDF Files with ThreadPoolExecutor:\n",
    "def process_multiple_pdfs(pdf_folder_path: str, max_workers: int = 4) -> List:\n",
    "    \"\"\"Processes all PDF files within a specified folder using ThreadPoolExecutor.\n",
    "\n",
    "    Logs information and errors during processing.\n",
    "    \"\"\"\n",
    "    all_elements = []\n",
    "    filenames = [filename for filename in os.listdir(pdf_folder_path) if filename.endswith(\".pdf\")]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = executor.map(lambda filename: process_single_pdf(pdf_folder_path, filename), filenames)\n",
    "\n",
    "    for result in results:\n",
    "        all_elements.extend(result)  # Extend the all_elements list\n",
    "\n",
    "    return all_elements\n",
    "\n",
    "# pdf_folder_path = \"/Users/nathan/workspace/tcc/app/src/database/pdf/\"\n",
    "# raw_pdfs_elements = process_multiple_pdfs(pdf_folder_path, max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = \"/Users/nathan/workspace/tcc/test\"\n",
    "raw_pdfs_elements = process_multiple_pdfs(pdf_folder_path, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(raw_pdfs_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pdfs_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store counts of each type\n",
    "category_counts = {}\n",
    "\n",
    "for element in raw_pdfs_elements:\n",
    "    category = str(type(element))\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# Unique_categories will have unique elements\n",
    "unique_categories = set(category_counts.keys())\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the Element class based on potential types returned by partition_pdf\n",
    "class Element(BaseModel):\n",
    "    type: str  # Textual representation of the element type (e.g., \"table\", \"text\")\n",
    "    text: Any  # Content of the element, can be text, tables, or other data structures\n",
    "\n",
    "# Categorize by type\n",
    "def categorize_elements(raw_pdf_elements) -> list[Element]:\n",
    "    \"\"\"Categorizes elements by type and returns a dictionary with counts.\"\"\"\n",
    "    categorized_elements = [] # Initialize category counts\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            categorized_elements.append(Element(type=\"table\", text=str(element)))\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            categorized_elements.append(Element(type=\"text\", text=str(element)))\n",
    "    return categorized_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved categorization function with clear structure\n",
    "all_categories = categorize_elements(raw_pdfs_elements)\n",
    "print(len(all_categories))\n",
    "\n",
    "# Tables\n",
    "table_elements = [e for e in all_categories if e.type == \"table\"]\n",
    "print(f\"Number of tables: {len(table_elements)}\")\n",
    "\n",
    "# Text\n",
    "text_elements = [e for e in all_categories if e.type == \"text\"]\n",
    "print(f\"Number of text elements: {len(text_elements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_elements[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    # Remove linhas compostas apenas por espaços e caracteres de controle\n",
    "    text = re.sub(r'^\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Substitui múltiplos espaços em branco e quebras de linha por um único espaço\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Remove espaços extras ao redor de pontuações\n",
    "    text = re.sub(r'\\s+([.,;:!?])', r'\\1', text)\n",
    "    text = re.sub(r'([.,;:!?])\\s+', r'\\1 ', text)\n",
    "\n",
    "    # Remove espaços extras no início e fim do texto\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_elements(text_elements):\n",
    "    \"\"\"Aplica a limpeza de texto a todos os elementos de texto de um PDF.\"\"\"\n",
    "    cleaned_text_elements = []\n",
    "    for element in text_elements:\n",
    "        cleaned_text = clean_text(element.text)\n",
    "        cleaned_text_elements.append(Element(type=element.type, text=cleaned_text))\n",
    "    return cleaned_text_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table_elements(text_elements):\n",
    "    \"\"\"Aplica a limpeza de texto a todos os elementos de texto de um PDF.\"\"\"\n",
    "    cleaned_table_elements = []\n",
    "    for element in text_elements:\n",
    "        cleaned_table = clean_text(element.text)\n",
    "        cleaned_table_elements.append(Element(type=element.type, text=cleaned_table))\n",
    "    return cleaned_table_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pdf_elements = clean_text_elements(text_elements)\n",
    "for element in cleaned_pdf_elements:\n",
    "    print(element['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pdf_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing tables and text chunks from pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('app/src/shared/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\",\n",
    "                 temperature=0, top_p=0.85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"Você é um assistente com a tarefa de resumir tabelas e textos de normas brasileiras sobre instalações elétricas.\n",
    "Faça um resumo sucinto da tabela ou do texto a seguir. Tabela ou texto: {element} \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(table_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_summaries) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisar os crunks pai com filho(summarized by gemini model)\n",
    "import pandas as pd\n",
    "\n",
    "text_data = {\"Texts\": texts, \"Text_summaries\": text_summaries}\n",
    "df = pd.DataFrame(text_data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", task_type=\"retrieval_document\")\n",
    "\n",
    "# df['Embeddings'] = df.apply(lambda row: gemini_embeddings.embed_query(row['Title'], row['Text']), axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries[30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add to vectorstore\n",
    "Use Multi Vector Retriever with summaries:\n",
    "\n",
    "- InMemoryStore stores the raw text, tables\n",
    "- vectorstore stores the embedded summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('app/src/shared/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", task_type=\"retrieval_document\")\n",
    "gemini_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = gemini_embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test without summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to tables\n",
    "tables = [i.text for i in table_elements]\n",
    "# Apply to texts\n",
    "texts = [i.text for i in text_elements]\n",
    "\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "page_content_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(texts)\n",
    "]\n",
    "\n",
    "page_content_tables = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(tables)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=page_content_texts,                 # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "vectorstore = Chroma.from_documents(\n",
    "                     documents=page_content_tables,  # Data\n",
    "                     embedding=gemini_embeddings,    # Embedding model\n",
    "                     persist_directory=\"./chroma_db\" # Directory to save data\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Load from disk\n",
    "vectorstore = Chroma(collection_name=\"data\", \n",
    "                        embedding_function=gemini_embeddings,\n",
    "                        persist_directory=persist_directory)\n",
    "\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever \n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Conexão ao banco de dados SQLite\n",
    "conn = sqlite3.connect('chroma_db/chroma.sqlite3')\n",
    "\n",
    "# Consulta SQL\n",
    "query = \"SELECT * FROM embeddings_queue\"\n",
    "\n",
    "# Executa a consulta e retorna um DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Exibe o DataFrame\n",
    "print(df)\n",
    "\n",
    "# Fecha a conexão\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(page_content_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(page_content_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Conexão ao banco de dados SQLite\n",
    "conn = sqlite3.connect('chroma_db/chroma.sqlite3')\n",
    "\n",
    "# Consulta SQL\n",
    "query = \"SELECT * FROM embedding_fulltext_search\"\n",
    "\n",
    "# Executa a consulta e retorna um DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Exibe o DataFrame\n",
    "print(df)\n",
    "\n",
    "# Fecha a conexão\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Conexão ao banco de dados SQLite\n",
    "conn = sqlite3.connect('chroma_db/chroma.sqlite3')\n",
    "\n",
    "# Consulta SQL\n",
    "query = \"SELECT * FROM embeddings_queue\"\n",
    "\n",
    "# Executa a consulta e retorna um DataFrame\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Exibe o DataFrame\n",
    "print(df)\n",
    "\n",
    "# Fecha a conexão\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão ao banco de dados SQLite\n",
    "conn = sqlite3.connect('chroma_db/chroma.sqlite3')\n",
    "\n",
    "tables = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "\n",
    "print(tables)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexão ao banco de dados SQLite\n",
    "conn = sqlite3.connect('chroma_db/chroma.sqlite3')\n",
    "\n",
    "for table_name in tables['name']:\n",
    "    # Gerar consulta para a tabela atual\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "    # Obter e exibir dados da tabela\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    print(f\"\\nDados da tabela {table_name}:\")\n",
    "    print(df.to_string())\n",
    " \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "# Load from disk\n",
    "vectorstore = Chroma(collection_name=\"summaries\", \n",
    "                        embedding_function=gemini_embeddings,\n",
    "                        persist_directory=persist_directory)\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "\n",
    "non_empty_summaries_texts = [summary for summary in summary_texts if summary.page_content.strip()]\n",
    "\n",
    "retriever.vectorstore.add_documents(non_empty_summaries_texts)\n",
    "# retriever.docstore.mset(list(zip(doc_ids, texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.docstore.mset(list(zip(doc_ids, texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "\n",
    "non_empty_summary_tables = [summary for summary in summary_tables if summary.page_content.strip()]\n",
    "\n",
    "retriever.vectorstore.add_documents(non_empty_summary_tables)\n",
    "# retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.docstore.mset(list(zip(table_ids, tables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Prompt template to query Gemini\n",
    "llm_prompt_template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use five sentences maximum and keep the answer concise.\\n\n",
    "Question: {question} \\nContext: {context} \\nAnswer:\"\"\"\n",
    "\n",
    "llm_prompt = PromptTemplate.from_template(llm_prompt_template)\n",
    "\n",
    "print(llm_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "vectorstore_disk = Chroma(\n",
    "                        persist_directory=\"./chroma_db\",       # Directory of db\n",
    "                        embedding_function=gemini_embeddings   # Embedding model\n",
    "                   )\n",
    "\n",
    "retriever = vectorstore_disk.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "# Combine data from documents to readable string format.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create stuff documents chain using LCEL.\n",
    "#\n",
    "# This is called a chain because you are chaining together different elements\n",
    "# with the LLM. In the following example, to create the stuff chain, you will\n",
    "# combine the relevant context from the website data matching the question, the\n",
    "# LLM model, and the output parser together like a chain using LCEL.\n",
    "#\n",
    "# The chain implements the following pipeline:\n",
    "# 1. Extract the website data relevant to the question from the Chroma\n",
    "#    vector store and save it to the variable `context`.\n",
    "# 2. `RunnablePassthrough` option to provide `question` when invoking\n",
    "#    the chain.\n",
    "# 3. The `context` and `question` are then passed to the prompt where they\n",
    "#    are populated in the respective variables.\n",
    "# 4. This prompt is then passed to the LLM (`gemini-pro`).\n",
    "# 5. Output from the LLM is passed through an output parser\n",
    "#    to structure the model's response.\n",
    "\n",
    "# Função para imprimir o contexto\n",
    "def print_context(context):\n",
    "    print(\"Contexto fornecido:\", context)\n",
    "    return context\n",
    "\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, top_p=0.85)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | RunnableMap({\"context\": print_context, \"question\": RunnablePassthrough()})\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"O que diz o topico 10.2.8.1 da norma regulamentadora NR 10?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, top_p=0.85)\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Você é um assistente chamado Spark e sua função é responder dúvidas e questionamentos relacionadas as instalações elétricas brasileiras com base no contexto, o qual pode incluir textos e/ou tabelas referentes as normas brasileiras (NBRs):\n",
    "{context}\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Função para imprimir o contexto\n",
    "def print_context(context):\n",
    "    print(\"Contexto fornecido:\", context)\n",
    "    return context\n",
    "\n",
    "# RAG pipeline com etapa intermediária para capturar e imprimir o contexto\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | RunnableMap({\"context\": print_context, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocar a chain e imprimir o resultado\n",
    "result = chain.invoke(\"O que diz o topico 10.2.8.1 da norma regulamentadora NR 10?\")\n",
    "print(\"Resposta:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocar a chain e imprimir o resultado\n",
    "result = chain.invoke(\"De acordo com a NBR 14039, na tabela 40 - Fatores de correção de agrupamento, qual é o fator de correção para o número de 9 condutores isolados?\")\n",
    "print(\"Resposta:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"De acordo com a NBR 14039, na tabela 40 - Fatores de correção de agrupamento, qual é o fator de correção para o número de 9 condutores isolados?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"Qual a definição de barramento blindado segundo a NBR 14039?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap\n",
    "\n",
    "model = GoogleGenerativeAI(model=\"gemini-pro\", temperature=0.5, top_p=0.85)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Você é um assistente chamado Spark e sua função é responder dúvidas e questionamentos relacionadas as instalações elétricas brasileiras com base no contexto, o qual pode incluir textos e/ou tabelas referentes as normas brasileiras (NBRs):\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Função para imprimir o contexto\n",
    "def print_context(context):\n",
    "    print(\"Contexto fornecido:\", context)\n",
    "    return context\n",
    "\n",
    "# RAG pipeline com etapa intermediária para capturar e imprimir o contexto\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | RunnableMap({\"context\": print_context, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Invocar a chain e imprimir o resultado\n",
    "result = chain.invoke(\"De acordo com a NBR 14039, na tabela 40 - Fatores de correção de agrupamento, qual é o fator de correção para o número de 9 condutores isolados?\")\n",
    "print(\"Resposta:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG pipeline with llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(\n",
    "    model=\"llama3\"\n",
    ")  # assuming you have Ollama installed and have llama3 model pulled with `ollama pull llama3 `\n",
    "\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt template\n",
    "template = \"\"\"Answer the question based only on the following context, which can include text and tables:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"De acordo com a NBR 14039, na tabela 39 - eletrodos de aterramento convencnionas, para o tipo de eletrodo de tubo de aço zincado, quais as dimensões mínimas?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(\"Qual foi minha pergunta anterior?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Chroma' from 'langchain_core.vectorstores' (/opt/homebrew/anaconda3/envs/tcc/lib/python3.12/site-packages/langchain_core/vectorstores.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocuments\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbeddings\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Chroma' from 'langchain_core.vectorstores' (/opt/homebrew/anaconda3/envs/tcc/lib/python3.12/site-packages/langchain_core/vectorstores.py)"
     ]
    }
   ],
   "source": [
    "from langchain_core import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import uuid\n",
    "\n",
    "# Função de embedding de exemplo\n",
    "def example_embedding_function(texts):\n",
    "    # Retorna um embedding fictício (deve ser substituído pela sua função de embedding real)\n",
    "    return [[float(i) for i in range(10)] for _ in texts]\n",
    "\n",
    "# Diretório onde o vectorstore será persistido\n",
    "persist_directory = \"chroma_db/test_summaries/test\"\n",
    "\n",
    "# Inicializando o vectorstore\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"test\",\n",
    "    embedding_function=example_embedding_function,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "teste_do_desespero = ['oioi_teste_', 'ola']\n",
    "id_key = 'docs_id_key'\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in teste_do_desespero]\n",
    "page_content_test = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(teste_do_desespero)\n",
    "]\n",
    "\n",
    "vectorstore.add_documents(page_content_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tcc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
